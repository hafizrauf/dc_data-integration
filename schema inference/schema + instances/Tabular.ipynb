{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19afec2bdb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#Set Seed\n",
    "import random\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(555)\n",
    "np.random.seed(555)\n",
    "torch.manual_seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ISE8Jzy3ufI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied:\n" 
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "\n",
    "#!unzip millestone_3.zip\n",
    "\n",
    "!pip install tf-estimator-nightly==2.8.0.dev2021122109\n",
    "\n",
    "!pip install pytorch_widedeep\n",
    "\n",
    "!pip install munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statistics import mean, median, variancefrom pytorch_widedeep import Trainer, Tab2Vec\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, WideDeep, TabNet, TabTransformer\n",
    "from pytorch_widedeep.metrics import Accuracy\n",
    "from pytorch_widedeep.datasets import load_adult\n",
    "import random\n",
    "random.seed(555) #\n",
    "np.random.seed(555)\n",
    "torch.manual_seed(555)\n",
    "\n",
    "\n",
    "\n",
    "file_n = pd.read_csv('TextPre.csv', sep=',', on_bad_lines='skip')\n",
    "# ......Choose table embedding model\n",
    "# TabTransformer, TabNet (output dim TabTransformer = 208, TabNet= 693)\n",
    "table_emb_model = 'TabNet'\n",
    "\n",
    "vectors = []\n",
    "labels = []\n",
    "texts = []\n",
    "dim_size = []\n",
    "\n",
    "error = 0\n",
    "for i in range(len(file_n['file_names'])):\n",
    "    df_ = pd.read_csv('Data/Tables/'+file_n['file_names'][i]+'.csv', sep=',', on_bad_lines='skip')\n",
    "    df_ = df_.replace('����|NULL|Null|null|&NBSP|&Nbsp|nbsp|nbsp;|????|�|Nbsp|nbsp','')\n",
    "    df_ = df_.replace('&nbsp;','',regex=True)\n",
    "    df_ = df_.replace('NaN','',regex=True)\n",
    "    \n",
    "  \n",
    "    # Identify category and continious data\n",
    "    cols = df_.columns\n",
    "    cont_cols = df_._get_numeric_data().columns \n",
    "    cat_cols = list(set(cols) - set(cont_cols))\n",
    "    \n",
    "        \n",
    "    # Fit the TabPreprocessor\n",
    "    tab_preprocessor = TabPreprocessor(cat_embed_cols=cat_cols, continuous_cols=cont_cols,scale=False)  \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        X_tab = tab_preprocessor.fit_transform(df_)\n",
    "    except:\n",
    "        error = error +1\n",
    "    else:\n",
    "        \n",
    "        # Define the model (and let's assume we train it)\n",
    "        \n",
    "        if table_emb_model == 'TabTransformer':\n",
    "            md = TabFastFormer\n",
    "        if table_emb_model == 'TabNet':\n",
    "            md = TabNet    \n",
    "       \n",
    "        \n",
    "        tabmodel = md(column_idx=tab_preprocessor.column_idx,\n",
    "                        cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "                        continuous_cols=tab_preprocessor.continuous_cols) #TabTransformer input_dim = 16\n",
    "        \n",
    "       \n",
    "        model = WideDeep(deeptabular=tabmodel, pred_dim =384)  \n",
    "        \n",
    "        # Emb in 128 dim \n",
    "        t2v = Tab2Vec(model, tab_preprocessor)\n",
    "        try:\n",
    "            X_vec = t2v.transform(df_) # df_\n",
    "        except:\n",
    "            error = error + 1\n",
    "        else:\n",
    "            m_size=X_vec.shape\n",
    "            print(X_vec.shape)\n",
    "            print(m_size[1])\n",
    "            dim_size.append(m_size[1])\n",
    "           # Compute mean to embedding and vector to list vectors\n",
    "            vectors.append(np.mean(X_vec ,axis=0))\n",
    "            labels.append(file_n['Labels'][i])\n",
    "            texts.append(file_n['Text'][i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum time occurred dim size = 693\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "dim_size=max(dim_size)\n",
    "print('Maximum time occurred dim size = ' + str(dim_size))\n",
    "#vectors\n",
    "dim = dim_size   # for TabTransformer dim_size-1,for TabNet only dim_size and dim TabTransformer = 208, TabNet= 693\n",
    "df = pd.DataFrame(vectors)\n",
    "df= df.iloc[: , :dim_size]\n",
    "df=df.interpolate(method='linear', limit_direction = 'both')\n",
    "vectors = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "def sdcn_preprocessing(x,y):\n",
    "    with open('X.txt', 'w', encoding='utf8') as data:\n",
    "        x_str = ''\n",
    "        for i, xx in enumerate(x):\n",
    "            x_str = x_str + str(xx[0])\n",
    "            for j in range(1, len(xx)-1):\n",
    "                x_str = x_str + ' ' + str(xx[j])\n",
    "            x_str = x_str + ' ' + str(xx[len(xx)-1]) + '\\n'\n",
    "        data.write(x_str)\n",
    "    with open('labels.txt', 'w', encoding='utf8') as label:\n",
    "        y_str = ''\n",
    "        for yy in y :\n",
    "            y_str = y_str + str(yy) + '\\n'\n",
    "        label.write(y_str)\n",
    "\n",
    "sdcn_preprocessing(vectors,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Millestone_3_rev2.ipynb",
   "provenance": [
    {
     "file_id": "1a0kqo-WJCuhaNa67etaTBzEj0dac8Sn9",
     "timestamp": 1652805990681
    },
    {
     "file_id": "/v2/external/notebooks/intro.ipynb",
     "timestamp": 1652306815587
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
